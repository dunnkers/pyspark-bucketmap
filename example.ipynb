{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to group DataFrame columns into buckets and map the buckets to different values üöÄ\n",
    "\n",
    "Have you ever heard of pyspark's [`Bucketizer`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Bucketizer.html)? It can be really useful! Although you perhaps won't need it for some simple transformation, it can be really useful for certain usecases. \n",
    "\n",
    "In this blogpost, we will:\n",
    "\n",
    "1. Explore the `Bucketizer` class\n",
    "2. Combine it with `create_map`\n",
    "3. Use a module so we don't have to write the logic ourselves üóùü•≥\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem\n",
    "\n",
    "First, let's boot up a local spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://84cdf0bb89c8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff5cc7bcd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have this dataset containing some persons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "people = spark.createDataFrame(\n",
    "    [\n",
    "        Row(age=12, name=\"Damian\"),\n",
    "        Row(age=15, name=\"Jake\"),\n",
    "        Row(age=18, name=\"Dominic\"),\n",
    "        Row(age=20, name=\"John\"),\n",
    "        Row(age=27, name=\"Jerry\"),\n",
    "        Row(age=101, name=\"Jerry's Grandpa\"),\n",
    "    ]\n",
    ")\n",
    "people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what we would like to do, is map each person's age to an age category.\n",
    "\n",
    "|age range|life phase|\n",
    "|-|-|\n",
    "|0 to 12|Child|\n",
    "|12 to 18|Teenager|\n",
    "|18 to 25|Young adulthood|\n",
    "|25 to 70|Adult|\n",
    "|70 and beyond|Elderly|\n",
    "\n",
    "How best to go about this?\n",
    "\n",
    "## Using pyspark UDF's\n",
    "A first option is to use a PySpark UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----------+\n",
      "|age|           name|life phase|\n",
      "+---+---------------+----------+\n",
      "| 12|         Damian|  Teenager|\n",
      "| 15|           Jake|  Teenager|\n",
      "| 18|        Dominic|     Adult|\n",
      "| 20|           John|     Adult|\n",
      "| 27|          Jerry|     Adult|\n",
      "|101|Jerry's Grandpa|   Elderly|\n",
      "+---+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def age_to_life_phase(age: int):\n",
    "    if age < 0:\n",
    "        return \"Not yet born\"\n",
    "    elif age < 12:\n",
    "        return \"Child\"\n",
    "    elif age < 18:\n",
    "        return \"Teenager\"\n",
    "    elif age < 70:\n",
    "        return \"Adult\"\n",
    "    else:  # age >= 70\n",
    "        return \"Elderly\"\n",
    "\n",
    "bucketed_udf = udf(age_to_life_phase, \"string\")\n",
    "people_with_phase_udf: DataFrame = people.withColumn(\"life phase\", bucketed_udf(\"age\"))\n",
    "people_with_phase_udf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But UDF's are sloooow üêå. To keep our edge, we'll need something faster üò≤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `Bucketizer` + `create_map`\n",
    "Luckily, there's pyspark's `Bucketizer`. It works like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----------+\n",
      "|age|           name|life phase|\n",
      "+---+---------------+----------+\n",
      "| 12|         Damian|       2.0|\n",
      "| 15|           Jake|       2.0|\n",
      "| 18|        Dominic|       3.0|\n",
      "| 20|           John|       3.0|\n",
      "| 27|          Jerry|       4.0|\n",
      "|101|Jerry's Grandpa|       5.0|\n",
      "+---+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "bucketizer = Bucketizer(\n",
    "    inputCol=\"age\",\n",
    "    outputCol=\"life phase\",\n",
    "    splits=[\n",
    "        -float(\"inf\"), 0, 12, 18, 25, 70, float(\"inf\")\n",
    "    ]\n",
    ")\n",
    "bucketed: DataFrame = bucketizer.transform(people)\n",
    "bucketed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We just put our ages in buckets, represented by numbers. Let's now map each bucket to a life phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+\n",
      "|age|           name|     life phase|\n",
      "+---+---------------+---------------+\n",
      "| 12|         Damian|       Teenager|\n",
      "| 15|           Jake|       Teenager|\n",
      "| 18|        Dominic|Young adulthood|\n",
      "| 20|           John|Young adulthood|\n",
      "| 27|          Jerry|          Adult|\n",
      "|101|Jerry's Grandpa|        Elderly|\n",
      "+---+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, create_map\n",
    "from typing import Dict\n",
    "from pyspark.sql.column import Column\n",
    "\n",
    "range_mapper = create_map(\n",
    "    [lit(0.0), lit(\"Not yet born\")]\n",
    "    + [lit(1.0), lit(\"Child\")]\n",
    "    + [lit(2.0), lit(\"Teenager\")]\n",
    "    + [lit(3.0), lit(\"Young adulthood\")]\n",
    "    + [lit(4.0), lit(\"Adult\")]\n",
    "    + [lit(5.0), lit(\"Elderly\")]\n",
    ")\n",
    "people_phase_column: Column = bucketed[\"life phase\"]\n",
    "people_with_phase: DataFrame = bucketed.withColumn(\n",
    "    \"life phase\", range_mapper[people_phase_column]\n",
    ")\n",
    "people_with_phase.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ Success!\n",
    "\n",
    "Using a combination of `Bucketizer` and `create_map`, we managed to map people's age to their life phases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pyspark-bucketmap`\n",
    "\n",
    "üéÅ As a bonus, I put all of the above in a neat little module, which you can install simply using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark-bucketmap in /usr/local/lib/python3.9/dist-packages (0.0.4)\n",
      "Requirement already satisfied: pyspark>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from pyspark-bucketmap) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.9/dist-packages (from pyspark-bucketmap) (1.23.1)\n",
      "Requirement already satisfied: overrides>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pyspark-bucketmap) (6.1.0)\n",
      "Requirement already satisfied: typing-utils>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from overrides>=4.0.0->pyspark-bucketmap) (0.1.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.9/dist-packages (from pyspark>=1.4.0->pyspark-bucketmap) (0.10.9.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark-bucketmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the splits and mappings like before. Each dictionary key is a mapping to the n-th bucket (for example, bucket 1 refers to the range `0` to `12`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits: List[float] = [-float(\"inf\"), 0, 12, 18, 25, 70, float(\"inf\")]\n",
    "mapping: Dict[int, Column] = {\n",
    "    0: lit(\"Not yet born\"),\n",
    "    1: lit(\"Child\"),\n",
    "    2: lit(\"Teenager\"),\n",
    "    3: lit(\"Young adulthood\"),\n",
    "    4: lit(\"Adult\"),\n",
    "    5: lit(\"Elderly\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, simply import `pyspark_bucketmap.BucketMap` and call `transform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|           name|          phase|\n",
      "+---------------+---------------+\n",
      "|         Damian|       Teenager|\n",
      "|           Jake|       Teenager|\n",
      "|        Dominic|Young adulthood|\n",
      "|           John|Young adulthood|\n",
      "|          Jerry|          Adult|\n",
      "|Jerry's Grandpa|        Elderly|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark_bucketmap import BucketMap\n",
    "from typing import List, Dict\n",
    "\n",
    "bucket_mapper = BucketMap(\n",
    "    splits=splits, mapping=mapping, inputCol=\"age\", outputCol=\"phase\"\n",
    ")\n",
    "phases_actual: DataFrame = bucket_mapper.transform(people).select(\"name\", \"phase\")\n",
    "phases_actual.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the module here:\n",
    "\n",
    "[https://github.com/dunnkers/pyspark-bucketmap](https://github.com/dunnkers/pyspark-bucketmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "\n",
    "Using `pyspark-bucketmap` results in ~~faster~~ code compared to using pyspark UDF's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 ms ¬± 813 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1000\n",
    "bucket_mapper = BucketMap(\n",
    "    splits=splits, mapping=mapping, inputCol=\"age\", outputCol=\"phase\"\n",
    ")\n",
    "phases_actual: DataFrame = bucket_mapper.transform(people).select(\"name\", \"phase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.3 ms ¬± 281 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1000\n",
    "bucketed_udf = udf(age_to_life_phase, \"string\")\n",
    "people_with_phase_udf: DataFrame = people.withColumn(\"life phase\", bucketed_udf(\"age\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|`pyspark-bucketmap`|pyspark UDF|\n",
    "|-|-|\n",
    "|17 ms ¬± 813 ¬µs per loop|12.3 ms ¬± 281 ¬µs per loop|\n",
    "\n",
    "mean ¬± std. dev. of 7 runs, 1,000 loops each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "It's a wrap! Hope you learned something useful.\n",
    "\n",
    "---\n",
    "\n",
    "Written by [Jeroen Overschie](https://jeroenoverschie.nl/), working at [GoDataDriven](https://godatadriven.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
