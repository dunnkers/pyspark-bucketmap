{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/15 08:36:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e5e232c6ee0f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>testing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff940f0a30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setMaster(\"local[*]\")\n",
    "    .setAppName(\"testing\")\n",
    "    .set(\"spark.sql.shuffle.partitions\", 1)\n",
    ")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(age=12, name=\"Damian\"),\n",
    "    Row(age=15, name=\"Jake\"),\n",
    "    Row(age=18, name=\"Dominic\"),\n",
    "    Row(age=20, name=\"John\"),\n",
    "    Row(age=27, name=\"Jerry\"),\n",
    "    Row(age=101, name=\"Jerry's Grandpa\"),\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from typing import Dict, Optional, Any\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.column import Column\n",
    "from itertools import chain\n",
    "\n",
    "class AgeToRange(Bucketizer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AgeToRange, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.setSplits([-float(\"inf\"), 0, 12, 18, 25, 70, float(\"inf\")])\n",
    "\n",
    "        self._mapping: Dict[int, Column] = {\n",
    "            0: sf.lit(\"Not yet born\"),\n",
    "            1: sf.lit(\"Child\"),\n",
    "            2: sf.lit(\"Teenager\"),\n",
    "            3: sf.lit(\"Young adulthood\"),\n",
    "            4: sf.lit(\"Adult\"),\n",
    "            5: sf.lit(\"Adult\"),\n",
    "        }\n",
    "\n",
    "        assert len(self._mapping) == len(self.getSplits()) - 1\n",
    "\n",
    "    def transform(self, dataset: SparkDataFrame, params: Optional[Any] = None) -> SparkDataFrame:\n",
    "        bucketed: SparkDataFrame = super().transform(dataset, params)\n",
    "        buckets: Column = bucketed[self.getOutputCol()]\n",
    "\n",
    "        # Map ranges\n",
    "        range_map = chain(*self._mapping.items())\n",
    "        range_mapper = sf.create_map([sf.lit(x) for x in range_map])\n",
    "        with_ranges = bucketed.withColumn(self.getOutputCol(), range_mapper[buckets])\n",
    "\n",
    "        return with_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+\n",
      "|age|           name|          phase|\n",
      "+---+---------------+---------------+\n",
      "| 12|         Damian|       Teenager|\n",
      "| 15|           Jake|       Teenager|\n",
      "| 18|        Dominic|Young adulthood|\n",
      "| 20|           John|Young adulthood|\n",
      "| 27|          Jerry|          Adult|\n",
      "|101|Jerry's Grandpa|          Adult|\n",
      "+---+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AgeToRange(inputCol=\"age\", outputCol=\"phase\").transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark counterexample using: UDF's & JOINs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
